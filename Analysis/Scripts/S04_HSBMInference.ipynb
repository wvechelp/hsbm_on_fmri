{"cells":[{"cell_type":"markdown","metadata":{"id":"qi9UhijHaSBS"},"source":["# Inference of hierarchical stochastic block models (hSBMs) on RS-fMRI data"]},{"cell_type":"markdown","source":["This Python-notebook is the second step in the analysis of RS-fMRI data with hierarchical stochastic block models. It is part of the analyses underlying the dissertation \"Topic modelling for the stratification of neurological patients\" written by W. Van Echelpoel (WVE) under supervision of prof. D. Marinazzo (DM) (Ghent University). The data has been provided by DM and has been pre-processed through another Python-notebook ('S02_DataSelection.ipynb'). The reason for using two different notebooks is related to the used OS: Data pre-processing could be done locally (Anaconda-distribution in Windows OS), yet inference of hSBM requires a specific module for which native installation on Windows is not supported. Hence, this notebook was developed to work in the Google Colab environment, relying on a Linux OS. It is advised to take this into account when willing to use this notebook to repeat our analyses or for performing other analyses.\n","\n","The notebook has been developed to work with the pre-processed data obtained through the notebook 'S02_DataSelection.ipynb' and stored in a specific folder structure. It is assumed that this folder structure is copied to GoogleDrive prior to attempting to read in this data. Alternative structures are possible, but require changes in the notebook. \n","\n","Visualisations of the obtained networks and cluster membership distributions are provided to get an insight, yet final graphs for cluster membership have been developed in R ('S05_GraphsMembership.R').\n","\n","This notebook is in many aspects based on:\n","- The documentation of graphtool: https://graph-tool.skewed.de/static/doc/index.html\n","- The tutorial of Valle et al. (2016): https://github.com/fvalle1/hSBM_Topicmodel\n","- The study of Valle et al. (2020): https://github.com/fvalle1/topicTCGA\n"],"metadata":{"id":"yTm49_c7Mmfx"}},{"cell_type":"markdown","metadata":{"id":"768hTVTraevV"},"source":["## Preparations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wwgrZq5paRVh"},"outputs":[],"source":["# First, create environment with required modules\n","!echo \"deb http://downloads.skewed.de/apt focal main\" >> /etc/apt/sources.list\n","!apt-key adv --keyserver keyserver.ubuntu.com --recv-key 612DEFB798507F25\n","!apt-get update\n","!apt-get install python3-graph-tool python3-matplotlib libcairo2-dev python3-cairo libgtk-3-dev\n","\n","# Colab uses a Python install that deviates from the system's! We need some workarounds.\n","!apt purge python3-cairo\n","!apt install libcairo2-dev pkg-config python3-dev\n","!pip install --force-reinstall pycairo\n","!pip install zstandard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"figXYHZRajRQ"},"outputs":[],"source":["# Preparation of analyses\n","%load_ext autoreload\n","%autoreload 2\n","\n","import os\n","import sys\n","import math\n","import random\n","import numpy as np\n","import pandas as pd\n","import pylab as plt\n","import graph_tool.all as gt\n","from google.colab import drive\n","from collections import defaultdict\n","from sklearn import metrics\n","%matplotlib inline "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NWiR7Gxral1s"},"outputs":[],"source":["# Connect with GoogleDrive\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"ikx-0OAeasZL"},"source":["## Loading the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pYWa3buoaoFU"},"outputs":[],"source":["# Prepare loading data from Drive (complete ('NoNa') versus all data)\n","path_data = '/content/gdrive/My Drive/Analysis/Data/02 Cleaned data'\n","fname_data = ['D_PearsonCoefficient_NoNa.txt', 'D_PearsonCoefficient.txt', \n","              'D_SelectedPairs_NoNa_75.txt', 'D_SelectedPairs_75.txt', \n","              'D_SelectedPairs_NoNa_50.txt', 'D_SelectedPairs_50.txt'][5]\n","filename = os.path.join(path_data,fname_data)\n","\n","# Read in data and extract info\n","corrDF = pd.read_csv(filename, sep = \";\", index_col = 0)\n","lst_patients = corrDF.index.values.tolist()\n","lst_pairs = corrDF.columns.values.tolist()\n","\n","# Define treshold (MANUALLY!) --> '1' is selected for dynamic threshold\n","n_lmtPearson = [0, 0.1, 0.25, 1][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qbyf_JUPVpvs"},"outputs":[],"source":["# Make use of dynamic threshold\n","if n_lmtPearson == 1:\n","    # Determine number of edges (1.5 times number of nodes) and derive index\n","    n_indx = round(3 * (len(lst_patients) + len(lst_pairs)) / 2.0) - 1\n","    n_lmtPearson = float(math.floor(1000 * sorted(abs(corrDF).stack().tolist(), reverse = True)[n_indx])) * 0.001\n","    print(n_lmtPearson)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gw34TdoHBSmw"},"outputs":[],"source":["# Select subset if interested (MANUALLY!)\n","# corrDF = corrDF.iloc[:120,] # Healthy\n","# corrDF = corrDF.iloc[120:170,] # Schizophrenia\n","# corrDF = corrDF.iloc[170:219,] # Bipolar\n","# corrDF = corrDF.iloc[219:,] # ADHD"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HV5M8uD6avwM"},"outputs":[],"source":["corrDF.shape"]},{"cell_type":"markdown","metadata":{"id":"I2_qYajK032t"},"source":["# Building the graph - manual creation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xeSne71s09Hc"},"outputs":[],"source":["# Create a graph\n","g = gt.Graph(directed=False)\n","# Define node properties\n","# name: patients - id, pair - 'pair'\n","# kind: patients - 0, pair - 1\n","name = g.vp[\"name\"] = g.new_vp(\"string\")\n","kind = g.vp[\"kind\"] = g.new_vp(\"int\")\n","eweight = g.ep[\"weight\"] = g.new_ep(\"double\")\n","\n","pats_add = defaultdict(lambda: g.add_vertex())\n","pair_add = defaultdict(lambda: g.add_vertex())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G0Q35X2F1B6e"},"outputs":[],"source":["# Add all patients first\n","for i_d in range(corrDF.shape[0]):\n","    patient = lst_patients[i_d]\n","    p = pats_add[patient]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iJQ4xYqm1Ou7"},"outputs":[],"source":["# Add all patients and pairs as nodes, Pearson as links\n","for i_d in range(corrDF.shape[0]):\n","    # Recall patient from vertex dictionary\n","    patient = lst_patients[i_d]\n","    p = pats_add[patient]\n","    name[p] = patient\n","    kind[p] = 0\n","\n","    # Add (or recall) ROI-pairs and add Pearson as edge weight\n","    for j_d in range(len(lst_pairs)):\n","      r = pair_add[lst_pairs[j_d]]\n","      name[r] = lst_pairs[j_d]\n","      kind[r] = 1\n","\n","      # Add weight to edge\n","      if abs(corrDF.iloc[i_d][j_d]) >= n_lmtPearson:\n","        e = g.add_edge(p, r)\n","        eweight[e] = corrDF.iloc[i_d][j_d]"]},{"cell_type":"markdown","metadata":{"id":"ws3AHkNybKub"},"source":["## Fit model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1pgadIXtlh91"},"outputs":[],"source":["# Extract vertex properties (patient or ROI-pair), as constraint for next step\n","# Both for the clustering (clabel) as for partition description length (pclabel)\n","clabel = g.vp['kind']\n","state_args = {'clabel': clabel, 'pclabel': clabel}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pWImJpTVc9rH"},"outputs":[],"source":["n_init = 1\n","v_mdl = []\n","n_mdl = np.inf\n","gt.seed_rng(32) ## seed for graph-tool's random number generator --> same results\n","\n","for i_n_init in range(n_init):\n","    print(i_n_init + 1)\n","    base_type = gt.BlockState\n","    state_tmp = gt.minimize_nested_blockmodel_dl(g, state_args=dict(\n","        base_type=base_type, **state_args, \n","        recs=[g.ep.weight], rec_types=[\"real-normal\"]))\n","    L = 0\n","    for s in state_tmp.levels:\n","          L += 1\n","          if s.get_nonempty_B() == 2:\n","                break\n","    state_tmp = state_tmp.copy(bs=state_tmp.get_bs()[:L] + [np.zeros(1)])\n","    print(state_tmp)\n","\n","    mdl_tmp = state_tmp.entropy()\n","    v_mdl.append(mdl_tmp)\n","    if mdl_tmp < n_mdl:\n","        n_mdl = 1.0*mdl_tmp\n","        state = state_tmp.copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dKCVATYOF8Ld"},"outputs":[],"source":["# Visualise inferred network structure\n","state.draw(layout = \"bipartite\", sample_edges = 1000)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jaUIs9ULF6EP"},"outputs":[],"source":["# Save graph if interested, name to be defined (MANUALLY!)\n","file_out = \"F_Temporary.png\"\n","path_output = '/content/gdrive/My Drive/Analysis/Data/03 Analysed data/NetworkGraphs'\n","state.draw(layout = \"bipartite\", sample_edges = 1000, \n","           output = os.path.join(path_output, file_out))"]},{"cell_type":"markdown","metadata":{"id":"EOqiFQAo4STL"},"source":["### Characteristics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JSaafAhD4KXX"},"outputs":[],"source":["# Extract characteristics (note reduction of MDL)\n","n_mdl*0.000001, L"]},{"cell_type":"code","source":["# Define level to be studied (MANUALLY!)\n","n_lvl = 0"],"metadata":{"id":"yRUKVuD-RA4I"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1uyWHSYp4P69"},"outputs":[],"source":["## Define group/cluster membership\n","# Extract number of patients, roi-pairs, and edges\n","D, V, N = int(np.sum(g.vp['kind'].a==0)), int(np.sum(g.vp['kind'].a==1)), int(g.num_edges())\n","\n","# Extract state at specific level\n","state_l = state.project_level(l=n_lvl).copy(overlap=True)\n","b = gt.contiguous_map(state_l.b)\n","\n","label_map = {}\n","for v in g.vertices():\n","    label_map[state_l.b[v]] = b[v]\n","state_l = state_l.copy(b=b)\n","\n","state_l_edges = state_l.get_edge_blocks() ## labeled half-edges\n","\n","weights = 'weight' in g.ep.keys()\n","\n","B = state_l.get_nonempty_B()\n","\n","n_wb = np.zeros((V, B))  ## number of half-edges incident on pair-node w and labeled as pair-group tw\n","n_db = np.zeros((D, B))  ## number of half-edges incident on patient-node d and labeled as patient-group td\n","n_dbw = np.zeros((D, B)) ## number of half-edges incident on patient-node d and labeled as pair-group td\n","\n","eweight = g.ep[\"weight\"]\n","\n","ze = gt.ungroup_vector_property(state_l_edges, [0,1])\n","for v1, v2, z1, z2, w in g.get_edges([ze[0], ze[1], eweight]):\n","    n_db[int(v1), int(z1)] += w\n","    n_dbw[int(v1), int(z2)] += w\n","    n_wb[int(v2 - D), int(z2)] += w\n","\n","ind_d = np.where(np.sum(n_db, axis=0) > 0)[0]\n","Bd = len(ind_d)\n","n_db = n_db[:, ind_d]\n","\n","ind_w = np.where(np.sum(n_wb, axis=0) > 0)[0]\n","Bw = len(ind_w)\n","n_wb = n_wb[:, ind_w]\n","\n","# Group memberships of each pair-node P(t_w | w) and patient node P(t_d | d)\n","p_tw_w = (n_wb / np.sum(n_wb, axis=1)[:, np.newaxis]).T\n","p_td_d = (n_db / np.sum(n_db, axis=1)[:, np.newaxis]).T"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y4UKve7I4Y4e"},"outputs":[],"source":["# Extract number of clusters\n","len(p_td_d)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IYj4WEzOFhMM"},"outputs":[],"source":["# Determine Normalised Mutual Information (NMI) statistic\n","# Note that this is not possible when dealing with subsets\n","v_true = [1 for i in range(120)] + [2 for i in range(50)] + [3 for i in range(49)] + [4 for i in range(40)]\n","v_pred = np.matmul(p_td_d.transpose(), [i + 1 for i in range(len(p_td_d))])\n","\n","n_nmi = metrics.normalized_mutual_info_score(v_true, v_pred) # Also in R possible, with 'variant = \"sqrt\"'\n","n_nmi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v6lBBGesFuKC"},"outputs":[],"source":["# Compare NMI with null-statistic (random sampling)\n","# Note that this is not possible when dealing with subsets\n","v_nmi_null = []\n","for i_d in range(10):\n","  random.seed(i_d)\n","  v_null = v_pred[random.sample([i for i in range(len(v_pred))], len(v_pred))]\n","  v_nmi_null.append(metrics.normalized_mutual_info_score(v_true, v_null))\n","\n","np.mean(v_nmi_null), np.std(v_nmi_null), n_nmi / np.mean(v_nmi_null)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"stayFhDb4czd"},"outputs":[],"source":["# Plot memberships in two separate figures\n","plt.figure(figsize=(15,4))\n","\n","plt.subplot(121)\n","plt.imshow(p_td_d,origin='lower',aspect='auto',interpolation='none')\n","plt.title(r'Participant group membership $P(bp | p)$')\n","plt.xlabel('Participant p (index)')\n","plt.yticks(ticks=range(len(p_td_d)),labels=range(1,len(p_td_d)+1))\n","plt.ylabel('Participant group, bp')\n","plt.colorbar()\n","\n","plt.subplot(122)\n","plt.imshow(p_tw_w,origin='lower',aspect='auto',interpolation='none')\n","plt.title(r'ROI-pair group membership $P(br | r)$')\n","plt.xlabel('ROI-pair r (index)')\n","plt.yticks(ticks=range(len(p_td_d)),labels=range(1,len(p_td_d)+1))\n","plt.ylabel('ROI-pair group, br')\n","plt.colorbar()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jIldgBbh4fHm"},"outputs":[],"source":["# Save participant membership if interested, name to be defined (MANUALLY!)\n","file_out = 'D_Temporary.txt'\n","path_output = '/content/gdrive/My Drive/Analysis/Data/03 Analysed data/MembershipData'\n","np.savetxt(os.path.join(path_output, file_out), p_td_d)"]}],"metadata":{"colab":{"provenance":[{"file_id":"1xzKEKnJhr6rHit6-1AdLBePgvWbsfx-O","timestamp":1686130230186}],"authorship_tag":"ABX9TyPbZgVNb4hU6vEDu4d0nyXW"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}